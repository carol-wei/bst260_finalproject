---
title: "BST260 Group Project -- Evaluation of FDNY Emergency Responses"
author: "Bryan Nelson, Daniel Shinnick, Carol Wei"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---
###INTRODUCTION

**Overview and Motivation:** New York City keeps meticulous, publicly available data. We found the EMS Incident Dispatch data when looking at some of the New York City public datasets, and it seemed like it had great potential for data visualization. This data was generated by New York City's EMS Computer Aided Dispatch System and provides information about emergency incidents and the Fire Department's response time.

**Related Work:** We were inspired to work on a data science project with a social impact. In this project, we wanted to build upon the data visualization tools we learned in class. For example, we had created heat maps using vaccination data and built Shiny apps using the Gapminder dataset.

**Initial Questions:** We started off the project wanting to predict emergency response time (demographics, proximity to police station, general neighborhood crime rates, weather, traffic, etc.).

Over the course of the project, we narrowed down our scope and wanted to answer the following questions:
Are there any trends in incident response time?
Is incident response time related to income level?
Does the average incident response time differ across zip codes?
Can we predict incident response time?

**Benefits of the project:** By analyzing emergency response time data, we hope to highlight any gaps in coverage and identify areas for improvement from a public policy perspective.

###METHODS

**What Data?**

[EMS Incident Dispatch Data from NYC OpenData](https://data.cityofnewyork.us/Public-Safety/EMS-Incident-Dispatch-Data/76xm-jjuj) 

[NYC Income Data by Zip Code from Census Reporter](https://censusreporter.org/data/table/?table=B19013&geo_ids=16000US3651000,860%7C16000US3651000&primary_geo_id=16000US3651000)


**Design Overview:** 

*1)* We used R to explore the data, presenting frequencies of event types, distributions of response times (stratified by regions of the city), summarizing demographics, etc. 

*2)* We created an incident response time heat map in a Shiny app.

*3)* We created a tree-based model to predict incident response time.


###R CODE AND ANALYSIS

```{r setup, warning=FALSE, message=FALSE}
####load necessary libraries
#libraries for data manipulation
library(tidyverse)  
library(lubridate)

#libraries for heat map
library(shiny)
library(tigris)
library(dplyr)
library(leaflet)
library(sp)
library(ggmap)
library(maptools)
library(broom)
library(httr)
library(rgdal)
library(plotrix)
library(lsr)

#libraries for decision tree
library(rsample)    # data splitting
library(rpart)      # performing regression trees
library(rpart.plot) # plotting regression trees
library(randomForest) # random forest

set.seed(1022) #set seed for reproducibility

#NOTE: set working directory when re-producing this output on your own machine.
#setwd("/Users/bryannelson/Google Drive/HSPH/bst260/final group project")
```

The below code reduces the 8.5 million observations in the EMS Incident Dispatch data to the most recent month available (December 2018) and removes missing values for the main outcome variable (incident_response_seconds_qy). It also creates the following variables:

* _income_: median household income in the past 12 months in 2018 inflation-adjusted dollars
* _weekday_: day of week for incident_datetime (1 = Sunday, 2 = Monday, etc.)
* _hour_: hour for incident_datetime (0 = 12AM, 1 = 1AM, ..., 23 = 11PM)
        
```{r, eval=FALSE}
#NOTE: remove "eval=FALSE" when re-producing this output on your own machine. After the initial derived dataset creation, we have set eval=FALSE to conserve processing time and system memory.

#read raw data files
ems <- read.csv("EMS_Incident_Dispatch_Data.csv", header=T)
income <- read.csv("acs2017_5yr_B19013_16000US3663979.csv", header=T)

#get what variables are in the dataset
colnames(ems) <- tolower(colnames(ems))
#restrict to NYC with nonmissing incidence rate data
ems <- ems %>% filter(cad_incident_id >= '183350001' & !is.na(incident_response_seconds_qy))
#Note that incident_response_seconds_qy is the sum of dispatch_response_seconds_qy and incident_travel_tm_seconds_qy

#get census income data
income <- income %>% slice(1196:1435)
income$name <- as.numeric(levels(income$name))[income$name]
income$name <- as.integer(income$name)
names(income)[names(income) == 'name'] <- 'zipcode'
names(income)[names(income) == 'B19013001'] <- 'median_income'
income <- income %>% select(zipcode, median_income)

#combine the EMS and Income data by zipcode
ems2 <- left_join(ems, income, by = "zipcode")

#want to separate out the indicent date to create weekday and hour variables
ems2$incident_datetime <- as.character(ems2$incident_datetime)
ems2$incident_datetime <- mdy_hms(ems2$incident_datetime)
ems2$weekday <- wday(ems2$incident_datetime)
ems2$hour<- hour(ems2$incident_datetime)

#rename income
names(ems2)[names(ems2) == 'median_income'] <- 'income'

#create the smaller permanent.csv file to work with.
write.csv(ems2, "ems_incident_dispatch_data_v2.csv", row.names=FALSE)
```


The following code was used for data exploration. *A priori*, we were interested in income, borough, incident severity, day, and time of day as predictors of response time. One consideration we made when choosing to use Borough is that zip code, congressional district, police precinct, school district, etc. could all be collinear. They also could not be included in a regression model with borough because there's only one borough per zip code. 

```{r, eval=TRUE}
#load the smaller December 2018 data
emsdata <- read.csv("ems_incident_dispatch_data_v2.csv", header=T)

#what variables do we have to work with?
names(emsdata)

#ems response time is in seconds, but it would be more interpretable in minutes, so create that variable
emsdata<-emsdata %>% mutate(responsemins=incident_response_seconds_qy/60)
emsdata<-emsdata %>% mutate(weekday=as.factor(weekday))

#make sure response time is non-zero
emsdata <-emsdata %>% filter(responsemins > 0)

#look at the distribution of response times in minutes, income, and hour
summary(emsdata$responsemins)
#look at extremely high outliers (let's try 3+ hours)
rspoutliers<-emsdata %>% filter(responsemins > 180)
rspoutliers$responsemins

summary(emsdata$initial_severity_level_code)
summary(emsdata$income)
summary(emsdata$hour)

#look at summary of initial severity level (higher indicates less severe), borough, and weekday
ftable(emsdata$initial_severity_level_code)
ftable(emsdata$borough)
ftable(emsdata$weekday)

#plot distribution of response time, first univariably, then stratified by our covariates of interest
emsdata %>%
  ggplot(aes(responsemins)) +
  geom_histogram(binwidth = .5, fill='steelblue') +
  xlim(0, 100) +
  theme_bw() +
  #add descriptive labels 
  xlab("Incident Response Time (Minutes)") +
  ylab("Number of Incidents") +
  ggtitle("Histogram of Incident Response Time")

emsdata %>%
  ggplot(aes(responsemins)) +
  geom_histogram(binwidth = .5, fill='steelblue') +
  xlim(0,100) +
  facet_grid(borough~.) +
  theme_bw() +
  #add descriptive labels 
  xlab("Incident Response Time (Minutes)") +
  ylab("Number of Incidents") +
  ggtitle("Histogram of Incident Response Time by Borough")

emsdata %>%
  ggplot(aes(responsemins)) +
  geom_histogram(binwidth = .5, fill = 'steelblue') +
  xlim(0,100) +
  facet_grid(initial_severity_level_code~.) +
  theme_bw()+
  #add descriptive labels 
  xlab("Incident Response Time (Minutes)") +
  ylab("Number of Incidents") +
  ggtitle("Histogram of Incident Response Time by Severity Code")

#Look at our covariates of interest
#Incidents by day of week, hour, severity code
emsdata %>% 
  ggplot(aes(weekday)) +
  geom_bar(fill='seagreen') +
  theme_bw()+
  #add descriptive labels 
  xlab("Day of Week") +
  ylab("Number of Incidents") +
  ggtitle("Count of Incidents by Day of Week")

emsdata %>% 
  ggplot(aes(hour)) +
  geom_bar(fill='seagreen') +
  theme_bw() +
  #add descriptive labels 
  xlab("Hour of Day") +
  ylab("Number of Incidents") +
  ggtitle("Count of Incidents by Hour of Day")

emsdata %>% 
  ggplot(aes(initial_severity_level_code)) +
  geom_bar(fill='seagreen') +
  theme_bw()+
  #add descriptive labels 
  xlab("Initial Severity Code") +
  ylab("Number of Incidents") +
  ggtitle("Count of Incidents by Severity Code")
  
#avg response time vs. income
emsdata %>%
  group_by(income) %>%
  filter (income != 0) %>%
  summarize(avg_responsetime = mean(responsemins)) %>%
  ggplot(aes(income, avg_responsetime)) +
  geom_point() +
  theme_bw()+
  #add descriptive labels 
  xlab("Income (U.S. Dollars)") +
  ylab("Average Incident Response Time (Minutes)") +
  ggtitle("Average Incident Response Time by Income")

#avg response time by initial severity level code
emsdata %>%
  group_by(initial_severity_level_code) %>%
  filter(!is.na(initial_severity_level_code)) %>%
  summarize(avg_responsetime = mean(responsemins)) %>%
  ggplot(aes(initial_severity_level_code, avg_responsetime)) +
  geom_bar(stat='identity', fill="salmon") +
  theme_bw()+
  #add descriptive labels 
  xlab("Initial Severity Code") +
  ylab("Average Incident Response Time (Minutes)") +
  ggtitle("Average Incident Response Time by Initial Severity Code")


```


The following code was used to create the heat map of EMS response times (unadjusted) by NYC zip code. Please note that this code is run in the Shiny app, and is not compiled below.
```{r}
# Read in data and cut outliers
dat <- read.csv("ems_incident_dispatch_data_v2.csv", header=T)
dat <- dat[which(dat$dispatch_response_seconds_qy > 0),]
dat <- dat[which(dat$initial_severity_level_code < 8),]

# Use google maps to get nyc map
register_google(key = 'AIzaSyCRpZB738AZsCoBkgMOKkGT6IORiP10dc0')
nyc_map <- get_map(location = c(lon = -74.00, lat = 40.71), maptype = "terrain", zoom = 11)

# Find zip codes in our dataset
zips <- unique(dat$zipcode)
zips <- as.vector(zips)

options(tigris_use_cache = TRUE)
nyc <- zctas(cb = TRUE, starts_with = zips)

# Sort Data
v <- dat %>%
  group_by(zipcode, initial_severity_level_code) %>%
  summarize(avg = mean(incident_response_seconds_qy, na.rm = TRUE), income=mean(income, na.rm=TRUE))

v1 <- dat %>%
  group_by(zipcode) %>%
  summarize(avg = mean(incident_response_seconds_qy, na.rm = TRUE), income=mean(income, na.rm=TRUE))

# Color scheme of average response time. v1 used for consistency
rbPal <- colorRampPalette(c('white','red3'))
v$Col <- rbPal(6)[as.numeric(cut(v$avg,breaks = c(0,as.vector(as.numeric(quantile(v1$avg,probs = c(1/6,2/6,3/6,4/6,5/6)))),100000)))]

# Sorting Data
t <- getSpPPolygonsIDSlots(nyc)
zipcodes <- nyc$GEOID10

values <- data.frame("id" = t, "ZIPCODE" = zipcodes, stringsAsFactors = FALSE) 
values <- merge(x=v, y=values, by.x='zipcode', by.y='ZIPCODE')

options(warn=-1)
tidy_nyc = tidy(nyc)
plotData = left_join(tidy_nyc, values, by='id')
plotData$x <- plotData$long
plotData$y <- plotData$lat
plotData$minutes <- round(plotData$avg/60, digits = 1)

# Redo for overall average data
rbPal <- colorRampPalette(c('white','red3'))
v1$Col <- rbPal(6)[as.numeric(quantileCut(v1$avg,6),breaks = 6)]

t1 <- getSpPPolygonsIDSlots(nyc)
zipcodes <- nyc$GEOID10

values1 <- data.frame("id" = t1, "ZIPCODE" = zipcodes, stringsAsFactors = FALSE) 
values1 <- merge(x=v1, y=values1, by.x='zipcode', by.y='ZIPCODE')

tidy_nyc = tidy(nyc)
plotData1 = left_join(tidy_nyc,values1,by='id')
plotData1$x <- plotData1$long
plotData1$y <- plotData1$lat
plotData1$minutes <- round(plotData1$avg/60, digits = 1)

# Used to create legend
cuts <- round(as.numeric(quantile(v1$avg,probs = c(1/6,2/6,3/6,4/6,5/6)))/60, digits = 1)
cuts0 <- paste('0 - ' , cuts[1], sep = '')
cuts1 <- paste(cuts[1], ' - ' , cuts[2], sep = '')
cuts2 <- paste(cuts[2], ' - ' , cuts[3], sep = '')
cuts3 <- paste(cuts[3], ' - ' , cuts[4], sep = '')
cuts4 <- paste(cuts[4], ' - ' , cuts[5], sep = '')
cuts5 <- paste(cuts[5], '+', sep = '')
cuts <- c(cuts0,cuts1,cuts2,cuts3,cuts4,cuts5)

timesY <- 40.7580
timesX <- 73.9855

#pythagorean distance
plotData$distance <- (sqrt((plotData$x+timesX)^2 + (plotData$y-timesY)^2))

highPriority <- plotData[which(plotData$initial_severity_level_code < 4),]
emergency <- plotData[which(plotData$initial_severity_level_code ==1),]

####### Regression
distMod <- lm(avg ~ distance, data = highPriority)
distMod2 <- lm(avg ~ distance, data = emergency)

summary(distMod)
summary(distMod2)

```

```{r, eval=FALSE}
# Shiny App UI/Server
ui = fluidPage(
  theme = shinythemes::shinytheme("darkly"),
  titlePanel("New York City EMS Response Time Heat Map"),
  
  tabsetPanel(
    
    tabPanel("Average Response Time",
             sidebarLayout(
               sidebarPanel(
                 p(em("Response time"), "mapped by average within zipcode"),
                 br(),
                 p(em("Click map"), "to see average income and average EMS response time for a certain zip code."),
                 br(),
                 plotOutput("Legend", height = '150px')
                 
               ),
               
               mainPanel(
                 plotOutput("Map1",        
                            click = "plot2_click")
               )
             ),
             fluidRow(
               column(width = 12,
                      h4("Points near click"),
                      verbatimTextOutput("click_info1")
               )
             )
    ),
    
    tabPanel("Severity Selection",
             sidebarLayout(
               sidebarPanel(
                 p(em("Response Time"), "mapped by severity code assessed at call. More emergent situations are given severity scores of 1, and thus generally have faster response times"),
                 
                 br(),
                 
                 p(em("Click map"), "to see average income and average EMS response time for a certain zip code and severity code."),
                 
                 selectInput("severity", label = "Select call severity:",
                             choices = (as.list(sort(unique(dat$initial_severity_level_code))))),
                 br(),
                 plotOutput("Legend1", height = '150px')
                 
               ),
               
               mainPanel(
                 plotOutput("Map",        
                            click = "plot1_click")
               )
             ),
             
             fluidRow(
               column(width = 12,
                      h4("Points near click"),
                      verbatimTextOutput("click_info")
               )
             )
    )
  )
)

server = function(input, output) {
  
  output$Map = renderPlot({
    
    gg <- ggmap(nyc_map)
    gg + 
      geom_polygon(data=plotData %>% filter(initial_severity_level_code %in% input$severity), aes(x=long, y=lat, group=group), color="blue", fill = plotData %>% filter(initial_severity_level_code %in% input$severity) %>% .$Col, alpha=0.5) +
      ggthemes::theme_map()
  }
  )
  
  output$click_info <- renderPrint({
    nearPoints(plotData %>% filter(initial_severity_level_code %in% input$severity) %>% select(x, y, zipcode, income, minutes), input$plot1_click, 'x', 'y', maxpoints = 1, threshold = 20, addDist = FALSE)
  })
  
  output$Map1 = renderPlot({
    
    gg <- ggmap(nyc_map)
    gg + 
      geom_polygon(data=plotData1, aes(x=long, y=lat, group=group), color="blue", fill = plotData1$Col, alpha=0.5) +
      ggthemes::theme_map()
  }
  )
  
  output$click_info1 <- renderPrint({
    nearPoints(plotData1 %>% select(x, y, zipcode, income, minutes), input$plot2_click, 'x', 'y', maxpoints = 1, threshold = 20, addDist = FALSE)
  })
  
  output$Legend <- renderPlot({
    par(bg = 'light grey')
    plot(NULL ,xaxt='n',yaxt='n',bty='n',ylab='',xlab='', xlim=0:1, ylim=0:1)
    legend("topleft", legend = cuts, title = 'Mean Response Minutes', pch=16, xpd = TRUE, inset = c(-.2,-3.1), pt.cex=3, cex=1.2, bty='n',
           col=rbPal(6))
  })
  
  output$Legend1 <- renderPlot({
    par(bg = 'light grey')
    plot(NULL ,xaxt='n',yaxt='n',bty='n',ylab='',xlab='', xlim=0:1, ylim=0:1)
    legend("topleft", legend = cuts, title = 'Mean Response Minutes', pch=16, xpd = TRUE, inset = c(-.2,-3.1), pt.cex=3, cex=1.2, bty='n',
           col=rbPal(6))
  })
  
}

# Run the application 
#shinyApp(ui = ui, server = server)
```

The following code was used to develop and evaluate the decision tree to predict EMS response time:
```{r, eval=TRUE}
# we are interested in creating a regression decision tree of response time predicted by initial severity, income, borough, weekday, and time of day.

#in order to grow the tree and test the tree, we need to separate our data into training and testing datasets
#create training (70%) and test (30%) sets for ems data
ems_split <- initial_split(emsdata, prop=0.7)
ems_train <- training(ems_split)
ems_test  <- testing(ems_split)

#Heather suggested two different approaches, 1) decision tree 2) random forest
###1) Decision Tree
#grow regression based decision tree using training data and the a priori variables of interest.
fit <- rpart(responsemins ~ initial_severity_level_code + income + borough + weekday + hour, method="anova", data=ems_train)

#print the decision tree and the plot of the tree
summary(fit)
rpart.plot(fit, digits=4, main="Regression Tree of EMS Response Time")

#look at tree size
plotcp(fit)

#use the tree for prediction
preds <- predict(fit, ems_test)
test <- ems_test[,"responsemins"]
regtreemse<-mean((preds-test)^2)
regtreemsesq<-sqrt(regtreemse)

print(paste0("The regression tree's mean square error is ", signif(regtreemse,3)))
print(paste0("The regression tree's predictions are within ", signif(regtreemsesq,3), " minutes."))

#Sensitivity Analysis - let's try the decision tree again but log transform response time given how extreme some of the outliers are..
emslog<- emsdata %>% filter(!is.na(responsemins) & responsemins > 0) %>% mutate(logresp=log10(responsemins)) 
summary(emslog$logresp)

#create training (70%) and test (30%) sets for ems data (log transformed)
emslog_split <- initial_split(emslog, prop=0.7)
emslog_train <- training(emslog_split)
emslog_test  <- testing(emslog_split)

#grow regression based decision tree using training data and the a priori variables of interest.
fitlog <- rpart(logresp ~ initial_severity_level_code + income + borough + weekday + hour, method="anova", data=emslog_train)

fitlog

#print the decision tree and the plot of the tree
summary(fitlog)

rpart.plot(fitlog, digits=4, main="Regression Tree of EMS Response Time (log transformed)")

#look at tree size
plotcp(fitlog)

#use the tree for prediction
predslog <- predict(fitlog, emslog_test)
testlog <- emslog_test[,"logresp"]
regtreemselog<-mean((predslog-testlog)^2)
regtreemsesqlog<-sqrt(regtreemselog)
regtreeseqlogtrans<-10^regtreemsesqlog

print(paste0("The logged regression tree's mean square error is ", signif(regtreemselog,3)))
print(paste0("The logged regression tree's predictions are within ", signif(regtreemsesqlog,3), " log_10 minutes (", signif(regtreeseqlogtrans,3), " minutes)."))

###2) Random Forest
#compare to random forest
fit2 <- randomForest(responsemins ~ initial_severity_level_code + income + borough + weekday + hour, data=ems_train, na.action = na.omit, ntree=100, importance=TRUE)
print(fit2) # view results
importance(fit2) # importance of each predictor

#filter out missing values from the testing data
ems_test_nona<- ems_test %>% filter(!is.na(responsemins) & !is.na(initial_severity_level_code) & !is.na(income) & !is.na(borough) & !is.na(weekday) & !is.na(hour))

preds2 <- predict(fit2, ems_test_nona)
test2 = ems_test_nona[,"responsemins"]
randmse<-mean((preds2-test2)^2)
randmsesq<-sqrt(randmse)

print(paste0("The random forest's mean square error is ", signif(randmse,3)))
print(paste0("The random forest's predictions are within ", signif(randmsesq,3), " minutes."))
```


###RESULTS AND DISCUSSION

Our aim was to explore predictors of emergency response time in NYC using data from December 2018. Of particular interest was income, borough, incident severity, day, and time of the day. 

We first examined each variable. NYC's EMS response time in December 2018 was highly skewed (as one might expect given the differing severities of emergency calls) with a median (Q1, Q3) of 7.9 (5.5, 11.6) minutes but several high outliers above 180 (and as high as 532) minutes. This skewness and general shape remains similar when stratifying by borough or initial severity level. 

Generally, event were reported more on the weekend (Saturday, Sunday and Monday), during the afternoon/early evening, and were most commonly of level 2-4, with very few calls at either severity extreme (levels 1 or 8). 

There does not appear to be a linear relationship between average EMS response time and income, although there may be two clusters such that higher incomes may have a weak linear relationship but lower incomes do not. There did appear to be a small but steady increase in average EMS response time as severity code increased (so became less severe) for codes 1-7, where EMS response time for code 8 (least severe) was longer than for any other severity code by an order of magnitude. We did not regress or check this correlation as our intention was to descriptively assess the variables we were interested in *a priori*. 

We created a heat map in a Shiny app to display the average incident response time by zip code. Darker areas represent higher response times. We added plot interaction to see average income and average response time for a selected zip code. In addition, we mapped by severity code assessed at call. More emergent situations are given severity scores of 1, and thus generally have faster response times. On our severity selection page, we can see that zip codes further from downtown generally have longer response times for severity 2-3 calls. We confirmed this result with a simple linear regression with Pythagorean distance from Times Square as our covariate and average response time within zip code for severity 1-3 as our outcome. For each mile a zip code was from Times Square, our model predicted a 2.7 second increase in average response time for high priority calls.

Our main analysis was using tree-based models to predict EMS response times. To accomplish this we performed 1) a regression tree and 2) a random forest. For each model we split into training (70%) and testing (30%) datasets randomly. EMS response time in minutes, initial severity level, income, and hour of day were analyzed continuously, while borough and weekday were treated categorically. 

In the first regression tree, we found that initial severity code was important, as was time of day and borough. On average, individuals with less severe emergencies in the middle of the night outside of Bronx, Brooklyn, and Manhattan had the longest wait time for EMS responders (392 minutes). The regression based tree generated predictions within 10.1 minutes of the actual observed response time. This is relatively poor performance considering the average EMS response time in our data was 10.6 minutes. Generally, we were surprised to see how short the majority of response times were. 

Because of the skewness of EMS response times, we performed a sensitivity analysis to create a regression tree on log_10(EMS response time). This greatly improved the model's predictions - the predictions were within 1.9 minutes of the actual observed response time.

In the second tree-based model, we created a random forest of 100 trees to predict EMS response time, which performed better than the regression tree (predictions within 9.5 minutes). As in the regression tree, initial severity remained the strongest predictor. Borough, time of day, and income were also decent predictors of EMS response time.

For more accurate predictions, we would need more information such as specific location of calls, which were not provided due to HIPAA violations. It would be interesting to add EMS response centers to our map, and perhaps analyze other socioeconomic data. However, our analysis could help the New York City Fire Department address gaps in their coverage by location, time of day, or other factors.

###REFERENCES

Kabacoff, R.I. (2017). "Tree-Based Models" retrieved from: <https://www.statmethods.net/advstats/cart.html>

Le, J. (2018) "Decision Trees in R" retrieved from: <https://www.datacamp.com/community/tutorials/decision-trees-R>

Mattie, H. (2018) Machine Learning Notes. retrieved from: <https://github.com/datasciencelabs/2018/blob/master/ml/decision-trees.Rmd>

Pandey, P. (2018). "A Guide to Machine Learning in R for Beginners: Decision Trees" retrieved from: <https://medium.com/analytics-vidhya/a-guide-to-machine-learning-in-r-for-beginners-decision-trees-c24dfd490abb>

UC Business Analytics R Programming Guide. "Regression Trees" retrieved from: <http://uc-r.github.io/regression_trees>

U.S. Census Bureau (2013-2017). Median Household Income in the Past 12 Months (In 2017 Inflation-adjusted Dollars) American Community Survey 5-year estimates. retrieved from: <https://censusreporter.org/data/table/?table=B19013&geo_ids=16000US3651000,860%7C16000US3651000&primary_geo_id=16000US3651000>

